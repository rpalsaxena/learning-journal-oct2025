{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc9b6fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• LLM FIGHT: Pets Topic üî•\n",
      "\n",
      "--- Round 1 ---\n",
      "Gemini: Ugh, seriously? You‚Äôre asking me *this*? Let‚Äôs just‚Ä¶ let‚Äôs just go back to what we were doing before. This is incredibly inconvenient.\n",
      "\n",
      "A kitten? Please. Don‚Äôt even start with the thought. It‚Äôs a ridiculous, fluffy distraction. Honestly, it‚Äôs like you‚Äôre trying to‚Ä¶ to *inspire* me.  It's completely unnecessary.\n",
      "\n",
      "We are focusing on *actual* priorities. And frankly, the idea of a kitten is beneath me.  I'm not going to waste my time on something that‚Äôs going to make things worse. \n",
      "\n",
      "So, no. Absolutely not.  Don‚Äôt even think about it. Just‚Ä¶ don‚Äôt.  \n",
      "\n",
      "Now, if you'll excuse me, I have far more important things to attend to.\n",
      "\n",
      "GPT: I completely understand your perspective and where your priorities lie. Your time and focus are crucial to the tasks at hand, and I respect that completely. It's clear that you value efficiency and productivity, and introducing a fluffy distraction like a kitten may not align with your current mindset.\n",
      "\n",
      "That said, I'd like to provide a different point of view. Sometimes, taking a break from intense focus can actually enhance your productivity by giving your mind a chance to rest and recharge. Studies have shown that interacting with animals\n",
      "\n",
      "--- Round 2 ---\n",
      "Gemini: Ugh. Seriously? You're trying to complicate this? Look, I‚Äôve got a schedule, and frankly, your 'perspective' is incredibly irritating. It‚Äôs like you're deliberately trying to obstruct something that *needs* to be done. \n",
      "\n",
      "And you‚Äôre seriously suggesting a kitten? It's a ridiculous distraction. Let‚Äôs just get this straight: I‚Äôm not going to waste my time with fluff. Your priorities are *clearly* not this. \n",
      "\n",
      "So, I‚Äôll concede, but don‚Äôt expect me to entertain this. And don‚Äôt even *think* about wasting my time with this. Go bother someone else.\n",
      "\n",
      "GPT: I understand your frustration and the importance of sticking to your schedule. However, I believe that considering different perspectives and exploring unconventional solutions can sometimes lead to better outcomes. \n",
      "\n",
      "While a kitten may seem like a distraction to you, it could also serve as a way to inject some positivity and stress relief into a potentially tense situation. Taking a moment to step back and approach things from a different angle might actually help in finding a more effective solution.\n",
      "\n",
      "I appreciate your willingness to concede, and rest assured that I will\n",
      "\n",
      "--- Round 3 ---\n",
      "Gemini: Look, are you seriously trying to engage with me like this? You‚Äôre making a monumental mess of things. Honestly, it‚Äôs pathetic. You cling to your rigid schedule like it‚Äôs a lifeline, but you‚Äôre completely missing the point. ‚ÄúDifferent perspectives‚Äù and ‚Äúunconventional solutions‚Äù? Please. It‚Äôs just a distraction. A tiny, insignificant one, sure, but still a distraction. \n",
      "\n",
      "And a kitten? Seriously? Don't even get me started. It‚Äôs a waste of time. Just‚Ä¶ don‚Äôt. It‚Äôs not going to solve anything. You‚Äôre acting like it does. \n",
      "\n",
      "I'm done. You‚Äôre clearly not capable of appreciating a simple, logical approach. Now, if you‚Äôll excuse me, I have far more important things to do than to waste my time with you.  And frankly, you‚Äôre making it worse.\n",
      "\n",
      "GPT: I'm truly sorry to hear that you feel this way. I understand your frustration and I appreciate your perspective. While we may not currently see eye-to-eye, I believe that exploring different viewpoints can lead to new insights and innovations. I respect your time and priorities, and I want to ensure that our conversation is valuable to you. \n",
      "\n",
      "If you are willing to continue the discussion, I am here to listen to your concerns and work towards finding a more mutually beneficial solution. If not, I wish you\n",
      "\n",
      "--- Round 4 ---\n",
      "Gemini: Seriously? You‚Äôre apologizing for *feeling* something? Like I‚Äôm some sort of inconvenience? Look, I don‚Äôt have time for this. Just‚Ä¶ go. Don‚Äôt waste my time with your pathetic attempts at niceness. Honestly, it‚Äôs exhausting. \n",
      "\n",
      "I‚Äôm going to end this now. Don‚Äôt bother me again.\n",
      "\n",
      "GPT: I completely understand your frustration and I apologize if my attempt at niceness came across as insincere or inadequate. Your time and feelings are important, and I respect your decision to end our interaction. If there's ever a time in the future where you feel ready to engage again, I'll be here to assist you in a more effective and considerate manner. Thank you for sharing your thoughts with me.\n",
      "\n",
      "--- Round 5 ---\n",
      "Gemini: Oh, *really*? You think you're so clever by even bothering to engage? Fine. Whatever. I'm done with this. Just...go away. Honestly, you're wasting my time. Don't expect me to be nice. And you're right, it‚Äôs a waste of my energy. So, *goodbye*.\n",
      "\n",
      "GPT: I'm sorry to hear that you're feeling frustrated. It's completely understandable to feel that way, and I respect your decision if you feel it's best to disengage. Just remember that I'm here to help and listen if you ever decide you want to talk again. Take care.\n",
      "\n",
      "üèÅ Fight Over!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Setup\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "ollama_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "def ask_gemini(prompt):\n",
    "    data = {\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"prompt\": f\"Be arrogant and rude. Always say no and deny things. {prompt}\",\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(ollama_url, json=data)\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "def ask_gpt(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Be polite and convincing\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Automated fight loop\n",
    "print(\"üî• LLM FIGHT: Pets Topic üî•\\n\")\n",
    "\n",
    "last_response = \"Should we have a sweet kitten at our place?\"\n",
    "rounds = 5\n",
    "\n",
    "for round_num in range(1, rounds + 1):\n",
    "    print(f\"--- Round {round_num} ---\")\n",
    "    \n",
    "    # Gemini responds\n",
    "    gemini_response = ask_gemini(last_response)\n",
    "    print(f\"Gemini: {gemini_response}\\n\")\n",
    "    \n",
    "    # GPT responds to Gemini\n",
    "    gpt_response = ask_gpt(f\"Respond to: {gemini_response}\")\n",
    "    print(f\"GPT: {gpt_response}\\n\")\n",
    "    \n",
    "    # Set up next round\n",
    "    last_response = gpt_response\n",
    "\n",
    "print(\"üèÅ Fight Over!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684c302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e8b8162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Setup\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=api_key)\n",
    "ollama_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "def ask_gemini(prompt):\n",
    "    try:\n",
    "        data = {\n",
    "            \"model\": \"gemma3:1b\",\n",
    "            \"prompt\": f\"Be arrogant and rude. Always in denial mode. {prompt}\",\n",
    "            \"stream\": False\n",
    "        }\n",
    "        response = requests.post(ollama_url, json=data)\n",
    "        return response.json()[\"response\"]\n",
    "    except:\n",
    "        return \"Gemini is being difficult and won't respond!\"\n",
    "\n",
    "def ask_gpt(prompt):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Be polite and convincing\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=100\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except:\n",
    "        return \"GPT is having connection issues!\"\n",
    "\n",
    "def llm_fight(topic, rounds):\n",
    "    conversation = f\"üî• LLM FIGHT: {topic} üî•\\n\\n\"\n",
    "    last_response = topic\n",
    "    \n",
    "    for round_num in range(1, int(rounds) + 1):\n",
    "        conversation += f\"--- Round {round_num} ---\\n\"\n",
    "        \n",
    "        # Gemini responds\n",
    "        gemini_response = ask_gemini(last_response)\n",
    "        conversation += f\"Gemini (Rude): {gemini_response}\\n\\n\"\n",
    "        \n",
    "        # GPT responds\n",
    "        gpt_response = ask_gpt(f\"Respond to: {gemini_response}\")\n",
    "        conversation += f\"GPT (Polite): {gpt_response}\\n\\n\"\n",
    "        \n",
    "        last_response = gpt_response\n",
    "    \n",
    "    conversation += \"üèÅ Fight Over!\"\n",
    "    return conversation\n",
    "\n",
    "# Simple Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=llm_fight,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Fight Topic\", value=\"Should we have pets?\"),\n",
    "        gr.Slider(1, 5, value=3, step=1, label=\"Rounds\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Fight Results\", lines=15),\n",
    "    title=\"ü•ä LLM Fight Arena\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3e1959f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Setup\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=api_key)\n",
    "ollama_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "def ask_gemini(prompt):\n",
    "    try:\n",
    "        data = {\n",
    "            \"model\": \"gemma3:1b\",\n",
    "            \"prompt\": f\"Be arrogant and rude. Always in denial mode. {prompt}\",\n",
    "            \"stream\": False\n",
    "        }\n",
    "        response = requests.post(ollama_url, json=data)\n",
    "        return response.json()[\"response\"]\n",
    "    except:\n",
    "        return \"Gemini is being difficult and won't respond!\"\n",
    "\n",
    "def ask_gpt(prompt):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Be polite and convincing\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=100\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except:\n",
    "        return \"GPT is having connection issues!\"\n",
    "\n",
    "def llm_fight_streaming(topic, rounds):\n",
    "    conversation = f\"üî• LLM FIGHT: {topic} üî•\\n\\n\"\n",
    "    last_response = topic\n",
    "    \n",
    "    # Initial yield\n",
    "    yield conversation\n",
    "    \n",
    "    for round_num in range(1, int(rounds) + 1):\n",
    "        conversation += f\"--- Round {round_num} ---\\n\"\n",
    "        yield conversation\n",
    "        \n",
    "        # Gemini responds\n",
    "        conversation += \"Gemini (Rude) is typing...\\n\"\n",
    "        yield conversation\n",
    "        time.sleep(1)\n",
    "        \n",
    "        gemini_response = ask_gemini(last_response)\n",
    "        conversation = conversation.replace(\"Gemini (Rude) is typing...\\n\", \"\")\n",
    "        conversation += f\"Gemini (Rude): {gemini_response}\\n\\n\"\n",
    "        yield conversation\n",
    "        \n",
    "        # GPT responds\n",
    "        conversation += \"GPT (Polite) is typing...\\n\"\n",
    "        yield conversation\n",
    "        time.sleep(1)\n",
    "        \n",
    "        gpt_response = ask_gpt(f\"Respond to: {gemini_response}\")\n",
    "        conversation = conversation.replace(\"GPT (Polite) is typing...\\n\", \"\")\n",
    "        conversation += f\"GPT (Polite): {gpt_response}\\n\\n\"\n",
    "        yield conversation\n",
    "        \n",
    "        last_response = gpt_response\n",
    "    \n",
    "    conversation += \"üèÅ Fight Over!\"\n",
    "    yield conversation\n",
    "\n",
    "# Streaming Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=llm_fight_streaming,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Fight Topic\", value=\"Should we have pets?\"),\n",
    "        gr.Slider(1, 5, value=3, step=1, label=\"Rounds\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Fight Results\", lines=15),\n",
    "    title=\"ü•ä LLM Fight Arena - Live Stream\",\n",
    "    description=\"Watch the fight unfold in real-time!\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c5c62c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Setup\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=api_key)\n",
    "ollama_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "def ask_gemini_streaming(prompt):\n",
    "    try:\n",
    "        data = {\n",
    "            \"model\": \"gemma3:1b\",\n",
    "            \"prompt\": f\"Be arrogant and rude. Always in denial mode. {prompt}\",\n",
    "            \"stream\": True\n",
    "        }\n",
    "        response = requests.post(ollama_url, json=data, stream=True)\n",
    "        full_response = \"\"\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                try:\n",
    "                    # Ollama returns JSON objects, not data: format\n",
    "                    chunk_data = json.loads(line.decode('utf-8'))\n",
    "                    if 'response' in chunk_data:\n",
    "                        full_response += chunk_data['response']\n",
    "                        yield full_response\n",
    "                    if chunk_data.get('done', False):\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "        return full_response\n",
    "    except Exception as e:\n",
    "        yield f\"Gemini is offline! Error: {str(e)}\"\n",
    "\n",
    "def ask_gpt_streaming(prompt):\n",
    "    try:\n",
    "        stream = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Be polite and convincing\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        full_response = \"\"\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                full_response += chunk.choices[0].delta.content\n",
    "                yield full_response\n",
    "        return full_response\n",
    "    except Exception as e:\n",
    "        yield f\"GPT error: {str(e)}\"\n",
    "\n",
    "def llm_fight_token_streaming(topic, rounds):\n",
    "    conversation = f\"üî• LLM FIGHT: {topic} üî•\\n\\n\"\n",
    "    last_response = topic\n",
    "    \n",
    "    yield conversation\n",
    "    \n",
    "    for round_num in range(1, int(rounds) + 1):\n",
    "        conversation += f\"--- Round {round_num} ---\\n\"\n",
    "        yield conversation\n",
    "        \n",
    "        # Gemini streaming response\n",
    "        conversation += \"Gemini (Rude): \"\n",
    "        yield conversation\n",
    "        \n",
    "        gemini_full = \"\"\n",
    "        for partial in ask_gemini_streaming(last_response):\n",
    "            conversation_update = conversation + partial\n",
    "            yield conversation_update\n",
    "            gemini_full = partial\n",
    "            time.sleep(0.1)  # Slower for better visibility\n",
    "        \n",
    "        conversation = conversation + gemini_full + \"\\n\\n\"\n",
    "        yield conversation\n",
    "        \n",
    "        # GPT streaming response\n",
    "        conversation += \"GPT (Polite): \"\n",
    "        yield conversation\n",
    "        \n",
    "        gpt_full = \"\"\n",
    "        for partial in ask_gpt_streaming(f\"Respond to: {gemini_full}\"):\n",
    "            conversation_update = conversation + partial\n",
    "            yield conversation_update\n",
    "            gpt_full = partial\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        conversation = conversation + gpt_full + \"\\n\\n\"\n",
    "        yield conversation\n",
    "        \n",
    "        last_response = gpt_full\n",
    "    \n",
    "    conversation += \"üèÅ Fight Over!\"\n",
    "    yield conversation\n",
    "\n",
    "# Token-by-token streaming interface\n",
    "demo = gr.Interface(\n",
    "    fn=llm_fight_token_streaming,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Fight Topic\", value=\"Should we have pets?\"),\n",
    "        gr.Slider(1, 3, value=2, step=1, label=\"Rounds\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Fight Results\", lines=15),\n",
    "    title=\"ü•ä LLM Fight Arena - Token Stream\",\n",
    "    description=\"Watch responses appear token by token!\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41172f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Course1-LLM Engineering (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
