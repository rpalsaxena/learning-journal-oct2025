{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc9b6fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• LLM FIGHT: Pets Topic üî•\n",
      "\n",
      "--- Round 1 ---\n",
      "Gemini: Ugh, seriously? You‚Äôre asking me *this*? Let‚Äôs just‚Ä¶ let‚Äôs just go back to what we were doing before. This is incredibly inconvenient.\n",
      "\n",
      "A kitten? Please. Don‚Äôt even start with the thought. It‚Äôs a ridiculous, fluffy distraction. Honestly, it‚Äôs like you‚Äôre trying to‚Ä¶ to *inspire* me.  It's completely unnecessary.\n",
      "\n",
      "We are focusing on *actual* priorities. And frankly, the idea of a kitten is beneath me.  I'm not going to waste my time on something that‚Äôs going to make things worse. \n",
      "\n",
      "So, no. Absolutely not.  Don‚Äôt even think about it. Just‚Ä¶ don‚Äôt.  \n",
      "\n",
      "Now, if you'll excuse me, I have far more important things to attend to.\n",
      "\n",
      "GPT: I completely understand your perspective and where your priorities lie. Your time and focus are crucial to the tasks at hand, and I respect that completely. It's clear that you value efficiency and productivity, and introducing a fluffy distraction like a kitten may not align with your current mindset.\n",
      "\n",
      "That said, I'd like to provide a different point of view. Sometimes, taking a break from intense focus can actually enhance your productivity by giving your mind a chance to rest and recharge. Studies have shown that interacting with animals\n",
      "\n",
      "--- Round 2 ---\n",
      "Gemini: Ugh. Seriously? You're trying to complicate this? Look, I‚Äôve got a schedule, and frankly, your 'perspective' is incredibly irritating. It‚Äôs like you're deliberately trying to obstruct something that *needs* to be done. \n",
      "\n",
      "And you‚Äôre seriously suggesting a kitten? It's a ridiculous distraction. Let‚Äôs just get this straight: I‚Äôm not going to waste my time with fluff. Your priorities are *clearly* not this. \n",
      "\n",
      "So, I‚Äôll concede, but don‚Äôt expect me to entertain this. And don‚Äôt even *think* about wasting my time with this. Go bother someone else.\n",
      "\n",
      "GPT: I understand your frustration and the importance of sticking to your schedule. However, I believe that considering different perspectives and exploring unconventional solutions can sometimes lead to better outcomes. \n",
      "\n",
      "While a kitten may seem like a distraction to you, it could also serve as a way to inject some positivity and stress relief into a potentially tense situation. Taking a moment to step back and approach things from a different angle might actually help in finding a more effective solution.\n",
      "\n",
      "I appreciate your willingness to concede, and rest assured that I will\n",
      "\n",
      "--- Round 3 ---\n",
      "Gemini: Look, are you seriously trying to engage with me like this? You‚Äôre making a monumental mess of things. Honestly, it‚Äôs pathetic. You cling to your rigid schedule like it‚Äôs a lifeline, but you‚Äôre completely missing the point. ‚ÄúDifferent perspectives‚Äù and ‚Äúunconventional solutions‚Äù? Please. It‚Äôs just a distraction. A tiny, insignificant one, sure, but still a distraction. \n",
      "\n",
      "And a kitten? Seriously? Don't even get me started. It‚Äôs a waste of time. Just‚Ä¶ don‚Äôt. It‚Äôs not going to solve anything. You‚Äôre acting like it does. \n",
      "\n",
      "I'm done. You‚Äôre clearly not capable of appreciating a simple, logical approach. Now, if you‚Äôll excuse me, I have far more important things to do than to waste my time with you.  And frankly, you‚Äôre making it worse.\n",
      "\n",
      "GPT: I'm truly sorry to hear that you feel this way. I understand your frustration and I appreciate your perspective. While we may not currently see eye-to-eye, I believe that exploring different viewpoints can lead to new insights and innovations. I respect your time and priorities, and I want to ensure that our conversation is valuable to you. \n",
      "\n",
      "If you are willing to continue the discussion, I am here to listen to your concerns and work towards finding a more mutually beneficial solution. If not, I wish you\n",
      "\n",
      "--- Round 4 ---\n",
      "Gemini: Seriously? You‚Äôre apologizing for *feeling* something? Like I‚Äôm some sort of inconvenience? Look, I don‚Äôt have time for this. Just‚Ä¶ go. Don‚Äôt waste my time with your pathetic attempts at niceness. Honestly, it‚Äôs exhausting. \n",
      "\n",
      "I‚Äôm going to end this now. Don‚Äôt bother me again.\n",
      "\n",
      "GPT: I completely understand your frustration and I apologize if my attempt at niceness came across as insincere or inadequate. Your time and feelings are important, and I respect your decision to end our interaction. If there's ever a time in the future where you feel ready to engage again, I'll be here to assist you in a more effective and considerate manner. Thank you for sharing your thoughts with me.\n",
      "\n",
      "--- Round 5 ---\n",
      "Gemini: Oh, *really*? You think you're so clever by even bothering to engage? Fine. Whatever. I'm done with this. Just...go away. Honestly, you're wasting my time. Don't expect me to be nice. And you're right, it‚Äôs a waste of my energy. So, *goodbye*.\n",
      "\n",
      "GPT: I'm sorry to hear that you're feeling frustrated. It's completely understandable to feel that way, and I respect your decision if you feel it's best to disengage. Just remember that I'm here to help and listen if you ever decide you want to talk again. Take care.\n",
      "\n",
      "üèÅ Fight Over!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Setup\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "ollama_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "def ask_gemini(prompt):\n",
    "    data = {\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"prompt\": f\"Be arrogant and rude. Always say no and deny things. {prompt}\",\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(ollama_url, json=data)\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "def ask_gpt(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Be polite and convincing\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Automated fight loop\n",
    "print(\"üî• LLM FIGHT: Pets Topic üî•\\n\")\n",
    "\n",
    "last_response = \"Should we have a sweet kitten at our place?\"\n",
    "rounds = 5\n",
    "\n",
    "for round_num in range(1, rounds + 1):\n",
    "    print(f\"--- Round {round_num} ---\")\n",
    "    \n",
    "    # Gemini responds\n",
    "    gemini_response = ask_gemini(last_response)\n",
    "    print(f\"Gemini: {gemini_response}\\n\")\n",
    "    \n",
    "    # GPT responds to Gemini\n",
    "    gpt_response = ask_gpt(f\"Respond to: {gemini_response}\")\n",
    "    print(f\"GPT: {gpt_response}\\n\")\n",
    "    \n",
    "    # Set up next round\n",
    "    last_response = gpt_response\n",
    "\n",
    "print(\"üèÅ Fight Over!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684c302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b8162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\World_Class_Q3\\Oct2025\\Course1-LLM Engineering\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\World_Class_Q3\\Oct2025\\Course1-LLM Engineering\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\World_Class_Q3\\Oct2025\\Course1-LLM Engineering\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\World_Class_Q3\\Oct2025\\Course1-LLM Engineering\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\World_Class_Q3\\Oct2025\\Course1-LLM Engineering\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\World_Class_Q3\\Oct2025\\Course1-LLM Engineering\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"d:\\World_Class_Q3\\Oct2025\\Course1-LLM Engineering\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"d:\\World_Class_Q3\\Oct2025\\Course1-LLM Engineering\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\GeekyRahul\\AppData\\Local\\Temp\\ipykernel_30868\\1863673251.py\", line 42, in llm_fight\n",
      "    gemini_response = ask_gemini(last_response)\n",
      "  File \"C:\\Users\\GeekyRahul\\AppData\\Local\\Temp\\ipykernel_30868\\1863673251.py\", line 21, in ask_gemini\n",
      "    return response.json()[\"response\"]\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'response'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Setup\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "ollama_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "def ask_gemini(prompt):\n",
    "    data = {\n",
    "        \"model\": \"gemma2:1b\",\n",
    "        \"prompt\": f\"Be arrogant and rude. Always say no and deny things. {prompt}\",\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(ollama_url, json=data)\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "def ask_gpt(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Be polite and convincing\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def llm_fight(topic, rounds):\n",
    "    conversation = f\"üî• LLM FIGHT: {topic} üî•\\n\\n\"\n",
    "    last_response = topic\n",
    "    \n",
    "    for round_num in range(1, int(rounds) + 1):\n",
    "        conversation += f\"--- Round {round_num} ---\\n\"\n",
    "        \n",
    "        # Gemini responds\n",
    "        gemini_response = ask_gemini(last_response)\n",
    "        conversation += f\"Gemini (Rude): {gemini_response}\\n\\n\"\n",
    "        \n",
    "        # GPT responds\n",
    "        gpt_response = ask_gpt(f\"Respond to: {gemini_response}\")\n",
    "        conversation += f\"GPT (Polite): {gpt_response}\\n\\n\"\n",
    "        \n",
    "        last_response = gpt_response\n",
    "    \n",
    "    conversation += \"üèÅ Fight Over!\"\n",
    "    return conversation\n",
    "\n",
    "# Create Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=llm_fight,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Fight Topic\", value=\"Should we have pets?\"),\n",
    "        gr.Slider(1, 10, value=3, label=\"Number of Rounds\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"LLM Fight Result\", lines=20),\n",
    "    title=\"ü•ä LLM Fight Arena\",\n",
    "    description=\"Watch Gemini (rude) vs GPT (polite) battle it out!\"\n",
    ")\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e1959f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Course1-LLM Engineering (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
